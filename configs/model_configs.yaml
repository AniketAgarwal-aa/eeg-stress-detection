# ====================================================
# MODEL-SPECIFIC CONFIGURATIONS
# ====================================================

RandomForest:
  n_estimators: 200
  max_depth: null
  min_samples_split: 2
  min_samples_leaf: 1
  max_features: "sqrt"
  bootstrap: true
  n_jobs: -1
  random_state: 42

XGBoost:
  n_estimators: 200
  max_depth: 6
  learning_rate: 0.05
  subsample: 0.8
  colsample_bytree: 0.8
  tree_method: "hist"
  eval_metric: "mlogloss"
  n_jobs: -1
  random_state: 42

SVM:
  kernel: "rbf"
  C: 2.0
  gamma: "scale"
  probability: true
  random_state: 42

LogisticRegression:
  penalty: "l2"
  C: 1.0
  max_iter: 1000
  n_jobs: -1
  random_state: 42

ExtraTrees:
  n_estimators: 200
  max_depth: null
  min_samples_split: 2
  min_samples_leaf: 1
  n_jobs: -1
  random_state: 42

LightGBM:
  n_estimators: 200
  max_depth: 6
  learning_rate: 0.05
  subsample: 0.8
  colsample_bytree: 0.8
  n_jobs: -1
  random_state: 42
  verbose: -1

KNN:
  n_neighbors: 5
  weights: "distance"
  algorithm: "auto"
  n_jobs: -1

GradientBoosting:
  n_estimators: 200
  max_depth: 6
  learning_rate: 0.05
  subsample: 0.8
  random_state: 42

# Deep Learning Models
eegnet:
  num_classes: 3
  channels: 62
  samples: 200
  dropout_rate: 0.5
  kernel_length: 64
  F1: 8
  D: 2
  F2: 16

eegnet_minimal:
  num_classes: 3
  channels: 62
  samples: 200
  dropout_rate: 0.25